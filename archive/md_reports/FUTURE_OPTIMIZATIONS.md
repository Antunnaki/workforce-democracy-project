# Future Optimizations & Enhancement Roadmap

**Project:** Workforce Democracy Backend  
**Document Created:** 2025-11-02  
**Last Updated:** 2025-11-02  
**Status:** Planning Phase

---

## ðŸ“‹ Overview

This document outlines planned optimizations and enhancements for the Workforce Democracy Project backend. All features described here are **not yet implemented** but have been identified as valuable improvements to system reliability, performance, and maintainability.

**Current System Status:** âœ… Fully operational with all core features working  
**Purpose of This Document:** Roadmap for future development sprints

---

## ðŸŽ¯ Priority Matrix

| Priority | Feature | Impact | Complexity | Estimated Time |
|----------|---------|--------|------------|----------------|
| HIGH | Database Integration (Redis) | High | Medium | 2-3 days |
| HIGH | Rate Limiting | High | Low | 1 day |
| MEDIUM | Monitoring (Prometheus/Grafana) | Medium | High | 3-5 days |
| MEDIUM | Testing Suite (Jest) | High | Medium | 2-4 days |
| LOW | Archive Organization | Low | Low | 30 minutes |

---

## 1. Database Integration - Redis Caching

### Current State
```javascript
// In-memory Map-based caching (volatile, single-server only)
const districtCache = new Map(); // Lost on server restart
const CACHE_TTL = 1000 * 60 * 60 * 24; // 24 hours
```

**Limitations:**
- âŒ Cache lost on server restart/crash
- âŒ Cannot scale horizontally (no shared cache between instances)
- âŒ No persistence across deployments
- âŒ No cache expiration guarantees (depends on manual TTL checks)

### Proposed Solution: Redis Integration

**Benefits:**
- âœ… Persistent caching survives restarts
- âœ… Distributed caching for horizontal scaling
- âœ… Built-in TTL expiration (automatic cleanup)
- âœ… Sub-millisecond read performance
- âœ… Can cache API responses, not just district mappings

### Implementation Plan

#### Step 1: Install Dependencies
```bash
cd /var/www/workforce-democracy/backend
npm install redis ioredis
```

#### Step 2: Create Redis Service Module
**File:** `/var/www/workforce-democracy/backend/redis-service.js`

```javascript
const Redis = require('ioredis');

class RedisService {
    constructor() {
        this.client = new Redis({
            host: process.env.REDIS_HOST || 'localhost',
            port: process.env.REDIS_PORT || 6379,
            password: process.env.REDIS_PASSWORD || null,
            retryStrategy: (times) => {
                const delay = Math.min(times * 50, 2000);
                return delay;
            },
            maxRetriesPerRequest: 3
        });

        this.client.on('error', (err) => {
            console.error('Redis Client Error:', err);
        });

        this.client.on('connect', () => {
            console.log('âœ… Redis connected successfully');
        });
    }

    /**
     * Get cached value with automatic JSON parsing
     * @param {string} key - Cache key
     * @returns {Promise<any|null>} Parsed value or null if not found
     */
    async get(key) {
        try {
            const value = await this.client.get(key);
            return value ? JSON.parse(value) : null;
        } catch (error) {
            console.error(`Redis GET error for key ${key}:`, error);
            return null;
        }
    }

    /**
     * Set cached value with TTL
     * @param {string} key - Cache key
     * @param {any} value - Value to cache (will be JSON stringified)
     * @param {number} ttlSeconds - Time to live in seconds (default 86400 = 24 hours)
     */
    async set(key, value, ttlSeconds = 86400) {
        try {
            await this.client.setex(key, ttlSeconds, JSON.stringify(value));
            return true;
        } catch (error) {
            console.error(`Redis SET error for key ${key}:`, error);
            return false;
        }
    }

    /**
     * Delete cached value
     * @param {string} key - Cache key to delete
     */
    async del(key) {
        try {
            await this.client.del(key);
            return true;
        } catch (error) {
            console.error(`Redis DEL error for key ${key}:`, error);
            return false;
        }
    }

    /**
     * Check if key exists
     * @param {string} key - Cache key
     * @returns {Promise<boolean>}
     */
    async exists(key) {
        try {
            const result = await this.client.exists(key);
            return result === 1;
        } catch (error) {
            console.error(`Redis EXISTS error for key ${key}:`, error);
            return false;
        }
    }

    /**
     * Get cache statistics
     */
    async getStats() {
        try {
            const info = await this.client.info('stats');
            const memory = await this.client.info('memory');
            return { info, memory };
        } catch (error) {
            console.error('Redis STATS error:', error);
            return null;
        }
    }

    /**
     * Graceful shutdown
     */
    async disconnect() {
        await this.client.quit();
    }
}

module.exports = new RedisService();
```

#### Step 3: Update us-representatives.js

**Replace in-memory cache with Redis:**

```javascript
// OLD CODE (remove this):
const districtCache = new Map();
const CACHE_TTL = 1000 * 60 * 60 * 24;

function getCachedDistrict(zipCode) {
    const cached = districtCache.get(zipCode);
    if (cached && Date.now() - cached.timestamp < CACHE_TTL) {
        return cached.data;
    }
    return null;
}

function cacheDistrict(zipCode, districtData) {
    districtCache.set(zipCode, {
        data: districtData,
        timestamp: Date.now()
    });
}

// NEW CODE (add this):
const redisService = require('./redis-service');

async function getCachedDistrict(zipCode) {
    const cacheKey = `district:${zipCode}`;
    return await redisService.get(cacheKey);
}

async function cacheDistrict(zipCode, districtData) {
    const cacheKey = `district:${zipCode}`;
    const TTL_24_HOURS = 86400; // seconds
    await redisService.set(cacheKey, districtData, TTL_24_HOURS);
}

// Update zipToCongressionalDistrict function:
async function zipToCongressionalDistrict(zipCode) {
    // Check cache first
    const cached = await getCachedDistrict(zipCode);
    if (cached) {
        console.log(`âœ… Cache hit for ZIP ${zipCode}`);
        return cached;
    }

    console.log(`âš ï¸  Cache miss for ZIP ${zipCode}, fetching from API...`);
    
    try {
        const apiUrl = `https://whoismyrepresentative.com/getall_mems.php?zip=${zipCode}&output=json`;
        const response = await fetch(apiUrl);
        const data = await response.json();
        
        const houseRep = data.results.find(r => r.district && r.district !== '');
        
        if (houseRep && houseRep.state && houseRep.district) {
            const districtData = {
                state: houseRep.state,
                district: parseInt(houseRep.district),
                zipCode,
                fallback: false
            };
            await cacheDistrict(zipCode, districtData);
            return districtData;
        }
        
        // Fallback logic...
    } catch (error) {
        console.error('Error in zipToCongressionalDistrict:', error);
        throw error;
    }
}
```

#### Step 4: Add Cache Management Endpoint

**In server.js, add cache monitoring endpoint:**

```javascript
// Cache statistics endpoint (admin only in production)
app.get('/api/admin/cache-stats', async (req, res) => {
    try {
        const redisService = require('./redis-service');
        const stats = await redisService.getStats();
        
        res.json({
            success: true,
            cacheType: 'Redis',
            stats: stats
        });
    } catch (error) {
        res.status(500).json({
            success: false,
            error: 'Failed to retrieve cache stats'
        });
    }
});

// Cache invalidation endpoint (admin only in production)
app.delete('/api/admin/cache/:zipCode', async (req, res) => {
    try {
        const { zipCode } = req.params;
        const redisService = require('./redis-service');
        
        await redisService.del(`district:${zipCode}`);
        
        res.json({
            success: true,
            message: `Cache cleared for ZIP ${zipCode}`
        });
    } catch (error) {
        res.status(500).json({
            success: false,
            error: 'Failed to clear cache'
        });
    }
});
```

#### Step 5: Update Environment Variables

**Add to .env:**
```bash
# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=  # Leave empty for local, set for production
```

#### Step 6: Install and Configure Redis Server

**On Ubuntu Server:**
```bash
# Install Redis
sudo apt update
sudo apt install redis-server -y

# Configure Redis for production
sudo nano /etc/redis/redis.conf

# Important settings to update:
# supervised systemd
# maxmemory 256mb
# maxmemory-policy allkeys-lru  # Evict least recently used keys when memory full
# bind 127.0.0.1  # Only allow local connections

# Enable and start Redis
sudo systemctl enable redis-server
sudo systemctl start redis-server

# Verify Redis is running
redis-cli ping
# Expected output: PONG
```

#### Step 7: Testing

**Test cache functionality:**
```bash
# Test 1: First request (cache miss)
time curl "https://api.workforcedemocracyproject.org/api/civic/representatives?zipCode=10001"
# Expected: ~2-3 seconds (API call)

# Test 2: Second request (cache hit)
time curl "https://api.workforcedemocracyproject.org/api/civic/representatives?zipCode=10001"
# Expected: ~200-500ms (Redis cache)

# Test 3: Check cache stats
curl "https://api.workforcedemocracyproject.org/api/admin/cache-stats"

# Test 4: Clear cache
curl -X DELETE "https://api.workforcedemocracyproject.org/api/admin/cache/:10001"

# Test 5: Verify cache cleared
redis-cli
> KEYS district:*
> GET district:10001
> TTL district:10001  # Should show seconds remaining until expiration
```

### Performance Gains Expected

| Metric | Before (In-Memory) | After (Redis) | Improvement |
|--------|-------------------|---------------|-------------|
| Cache persistence | Lost on restart | Survives restarts | âœ… Persistent |
| Response time (cached) | ~50ms | ~5ms | 10x faster |
| Horizontal scaling | Single server only | Multi-server shared cache | âœ… Scalable |
| Memory efficiency | Uncontrolled growth | LRU eviction policy | âœ… Controlled |

### Cost Estimation
- **Development Time:** 6-8 hours
- **Testing Time:** 2-3 hours
- **Redis Server:** Free (open source), ~10MB RAM usage for 10K ZIP cache
- **Maintenance:** Low (Redis is highly stable)

---

## 2. Rate Limiting Implementation

### Current State
**Problem:** No protection against API abuse, DDoS attacks, or accidental infinite loops from frontend bugs.

**Risks:**
- âŒ External API quotas exhausted (Congress.gov: 5,000/hour, Groq: 30/min)
- âŒ Server overload from malicious requests
- âŒ Unexpected costs if using paid API tiers

### Proposed Solution: Express Rate Limiting

**Benefits:**
- âœ… Protect external API quotas
- âœ… Prevent abuse and DDoS attacks
- âœ… Graceful degradation under heavy load
- âœ… Configurable per-endpoint limits

### Implementation Plan

#### Step 1: Install Dependencies
```bash
npm install express-rate-limit rate-limit-redis
```

#### Step 2: Create Rate Limiter Module
**File:** `/var/www/workforce-democracy/backend/rate-limiter.js`

```javascript
const rateLimit = require('express-rate-limit');
const RedisStore = require('rate-limit-redis');
const Redis = require('ioredis');

// Create Redis client for rate limiting (separate from cache)
const redisClient = new Redis({
    host: process.env.REDIS_HOST || 'localhost',
    port: process.env.REDIS_PORT || 6379,
    password: process.env.REDIS_PASSWORD || null
});

/**
 * General API rate limiter: 100 requests per 15 minutes per IP
 */
const generalLimiter = rateLimit({
    store: new RedisStore({
        client: redisClient,
        prefix: 'rl:general:'
    }),
    windowMs: 15 * 60 * 1000, // 15 minutes
    max: 100, // 100 requests per window
    message: {
        error: 'Too many requests from this IP, please try again later.',
        retryAfter: '15 minutes'
    },
    standardHeaders: true, // Return rate limit info in `RateLimit-*` headers
    legacyHeaders: false, // Disable `X-RateLimit-*` headers
    handler: (req, res) => {
        res.status(429).json({
            success: false,
            error: 'Rate limit exceeded',
            message: 'Too many requests. Please wait 15 minutes before trying again.',
            retryAfter: req.rateLimit.resetTime
        });
    }
});

/**
 * AI/LLM endpoint limiter: 10 requests per minute per IP
 * (More restrictive due to Groq API limits and computational cost)
 */
const aiLimiter = rateLimit({
    store: new RedisStore({
        client: redisClient,
        prefix: 'rl:ai:'
    }),
    windowMs: 60 * 1000, // 1 minute
    max: 10, // 10 requests per minute
    message: {
        error: 'AI query rate limit exceeded',
        retryAfter: '1 minute'
    },
    handler: (req, res) => {
        res.status(429).json({
            success: false,
            error: 'AI rate limit exceeded',
            message: 'Too many AI queries. Please wait 1 minute before asking another question.',
            retryAfter: req.rateLimit.resetTime
        });
    }
});

/**
 * Representatives lookup limiter: 30 requests per 5 minutes per IP
 * (Moderate restriction to prevent ZIP code enumeration attacks)
 */
const representativesLimiter = rateLimit({
    store: new RedisStore({
        client: redisClient,
        prefix: 'rl:reps:'
    }),
    windowMs: 5 * 60 * 1000, // 5 minutes
    max: 30, // 30 requests per window
    message: {
        error: 'Representative lookup rate limit exceeded',
        retryAfter: '5 minutes'
    }
});

/**
 * Health check limiter: 60 requests per minute per IP
 * (More permissive for monitoring tools)
 */
const healthCheckLimiter = rateLimit({
    store: new RedisStore({
        client: redisClient,
        prefix: 'rl:health:'
    }),
    windowMs: 60 * 1000, // 1 minute
    max: 60,
    skipSuccessfulRequests: true // Don't count successful requests
});

module.exports = {
    generalLimiter,
    aiLimiter,
    representativesLimiter,
    healthCheckLimiter
};
```

#### Step 3: Apply Rate Limiters to Endpoints

**Update server.js:**

```javascript
const { 
    generalLimiter, 
    aiLimiter, 
    representativesLimiter, 
    healthCheckLimiter 
} = require('./rate-limiter');

// Apply general rate limiter to all routes
app.use('/api/', generalLimiter);

// Apply specific rate limiters to endpoints
app.get('/health', healthCheckLimiter, (req, res) => {
    res.json({
        status: 'healthy',
        groqConfigured: !!process.env.GROQ_API_KEY,
        timestamp: new Date().toISOString(),
        uptime: process.uptime()
    });
});

app.get('/api/civic/representatives', representativesLimiter, async (req, res) => {
    // ... existing code ...
});

app.post('/api/groq/voting-assistant', aiLimiter, async (req, res) => {
    // ... existing code ...
});

app.post('/api/backend/query', aiLimiter, async (req, res) => {
    // ... existing code ...
});

app.get('/api/bills/search', representativesLimiter, async (req, res) => {
    // ... existing code ...
});

app.get('/api/supreme-court/cases', representativesLimiter, async (req, res) => {
    // ... existing code ...
});
```

#### Step 4: Add Rate Limit Monitoring Endpoint

```javascript
// Rate limit statistics (admin only)
app.get('/api/admin/rate-limits', async (req, res) => {
    try {
        const Redis = require('ioredis');
        const redis = new Redis({
            host: process.env.REDIS_HOST || 'localhost',
            port: process.env.REDIS_PORT || 6379
        });

        // Get all rate limit keys
        const generalKeys = await redis.keys('rl:general:*');
        const aiKeys = await redis.keys('rl:ai:*');
        const repsKeys = await redis.keys('rl:reps:*');

        res.json({
            success: true,
            rateLimits: {
                general: {
                    activeIPs: generalKeys.length,
                    limit: '100 requests per 15 minutes'
                },
                ai: {
                    activeIPs: aiKeys.length,
                    limit: '10 requests per minute'
                },
                representatives: {
                    activeIPs: repsKeys.length,
                    limit: '30 requests per 5 minutes'
                }
            }
        });

        redis.disconnect();
    } catch (error) {
        res.status(500).json({
            success: false,
            error: 'Failed to retrieve rate limit stats'
        });
    }
});
```

#### Step 5: Frontend Error Handling

**Update frontend to handle 429 responses gracefully:**

```javascript
// Example fetch with rate limit handling
async function fetchRepresentatives(zipCode) {
    try {
        const response = await fetch(`/api/civic/representatives?zipCode=${zipCode}`);
        
        if (response.status === 429) {
            const errorData = await response.json();
            
            // Show user-friendly message
            alert(`You've made too many requests. Please wait ${errorData.retryAfter} before trying again.`);
            
            // Optionally, implement automatic retry after delay
            const retryAfterMs = response.headers.get('Retry-After') * 1000;
            setTimeout(() => {
                console.log('Rate limit window expired, you can try again now');
            }, retryAfterMs);
            
            return null;
        }
        
        return await response.json();
    } catch (error) {
        console.error('Error fetching representatives:', error);
        return null;
    }
}
```

### Rate Limit Configuration Summary

| Endpoint | Window | Max Requests | Purpose |
|----------|--------|--------------|---------|
| `/api/*` (general) | 15 min | 100 | Prevent general API abuse |
| `/api/groq/*` (AI) | 1 min | 10 | Protect Groq API quota |
| `/api/backend/query` (AI) | 1 min | 10 | Protect Groq API quota |
| `/api/civic/representatives` | 5 min | 30 | Prevent ZIP enumeration |
| `/health` | 1 min | 60 | Allow monitoring tools |

### Testing Rate Limits

```bash
# Test 1: Hit AI endpoint rapidly (should block after 10 requests)
for i in {1..15}; do
    echo "Request $i:"
    curl -X POST https://api.workforcedemocracyproject.org/api/backend/query \
         -H "Content-Type: application/json" \
         -d '{"query":"Test","context":{}}' \
         -w "\nHTTP Status: %{http_code}\n\n"
    sleep 1
done
# Expected: First 10 succeed (200), remaining 5 fail (429)

# Test 2: Check rate limit headers
curl -I https://api.workforcedemocracyproject.org/api/civic/representatives?zipCode=10001
# Expected headers:
# RateLimit-Limit: 30
# RateLimit-Remaining: 29
# RateLimit-Reset: <timestamp>

# Test 3: Monitor rate limits
curl https://api.workforcedemocracyproject.org/api/admin/rate-limits
```

### Cost Estimation
- **Development Time:** 3-4 hours
- **Testing Time:** 1-2 hours
- **Redis Overhead:** Minimal (~1MB per 1000 active IPs)
- **Maintenance:** Low (automatic cleanup via TTL)

---

## 3. Monitoring with Prometheus & Grafana

### Current State
**Problem:** No visibility into:
- API response times
- Error rates
- Cache hit/miss ratios
- External API health
- Server resource usage

**Blind spots:**
- Can't detect performance degradation proactively
- No historical data for trend analysis
- Difficult to troubleshoot production issues

### Proposed Solution: Prometheus + Grafana Stack

**Benefits:**
- âœ… Real-time metrics dashboard
- âœ… Historical data storage (1 year+)
- âœ… Alerting on anomalies (email, Slack, PagerDuty)
- âœ… Custom business metrics (ZIP lookups per day, AI queries, etc.)

### Implementation Plan

#### Step 1: Install Dependencies
```bash
npm install prom-client express-prometheus-middleware
```

#### Step 2: Create Metrics Module
**File:** `/var/www/workforce-democracy/backend/metrics.js`

```javascript
const promClient = require('prom-client');

// Enable default metrics (CPU, memory, event loop lag)
promClient.collectDefaultMetrics({ timeout: 5000 });

// Custom metrics
const httpRequestDuration = new promClient.Histogram({
    name: 'http_request_duration_seconds',
    help: 'Duration of HTTP requests in seconds',
    labelNames: ['method', 'route', 'status_code'],
    buckets: [0.1, 0.5, 1, 2, 5, 10] // Response time buckets
});

const httpRequestTotal = new promClient.Counter({
    name: 'http_requests_total',
    help: 'Total number of HTTP requests',
    labelNames: ['method', 'route', 'status_code']
});

const cacheHitRate = new promClient.Counter({
    name: 'cache_operations_total',
    help: 'Total cache operations',
    labelNames: ['type', 'result'] // type: district|api, result: hit|miss
});

const externalApiDuration = new promClient.Histogram({
    name: 'external_api_duration_seconds',
    help: 'Duration of external API calls',
    labelNames: ['api_name', 'status'],
    buckets: [0.5, 1, 2, 5, 10, 30]
});

const externalApiErrors = new promClient.Counter({
    name: 'external_api_errors_total',
    help: 'Total external API errors',
    labelNames: ['api_name', 'error_type']
});

const aiQueryDuration = new promClient.Histogram({
    name: 'ai_query_duration_seconds',
    help: 'Duration of AI/LLM queries',
    labelNames: ['chat_type', 'model'],
    buckets: [1, 2, 5, 10, 20, 30, 60]
});

const zipLookupsByState = new promClient.Counter({
    name: 'zip_lookups_by_state_total',
    help: 'Total ZIP code lookups by state',
    labelNames: ['state']
});

const representativesReturned = new promClient.Histogram({
    name: 'representatives_returned_count',
    help: 'Number of representatives returned per query',
    buckets: [0, 1, 3, 5, 8, 10, 15]
});

// Middleware to track HTTP metrics
function metricsMiddleware(req, res, next) {
    const start = Date.now();
    
    res.on('finish', () => {
        const duration = (Date.now() - start) / 1000;
        const route = req.route ? req.route.path : req.path;
        
        httpRequestDuration.observe(
            { method: req.method, route, status_code: res.statusCode },
            duration
        );
        
        httpRequestTotal.inc({
            method: req.method,
            route,
            status_code: res.statusCode
        });
    });
    
    next();
}

// Helper functions for specific metrics
function recordCacheHit(type) {
    cacheHitRate.inc({ type, result: 'hit' });
}

function recordCacheMiss(type) {
    cacheHitRate.inc({ type, result: 'miss' });
}

function recordExternalApiCall(apiName, duration, success = true) {
    externalApiDuration.observe(
        { api_name: apiName, status: success ? 'success' : 'failure' },
        duration
    );
    
    if (!success) {
        externalApiErrors.inc({ api_name: apiName, error_type: 'request_failed' });
    }
}

function recordAiQuery(chatType, model, duration) {
    aiQueryDuration.observe({ chat_type: chatType, model }, duration);
}

function recordZipLookup(state, representativeCount) {
    zipLookupsByState.inc({ state });
    representativesReturned.observe(representativeCount);
}

module.exports = {
    promClient,
    metricsMiddleware,
    recordCacheHit,
    recordCacheMiss,
    recordExternalApiCall,
    recordAiQuery,
    recordZipLookup
};
```

#### Step 3: Integrate Metrics into Server

**Update server.js:**

```javascript
const { 
    promClient, 
    metricsMiddleware 
} = require('./metrics');

// Apply metrics middleware to all routes
app.use(metricsMiddleware);

// Expose metrics endpoint for Prometheus scraping
app.get('/metrics', async (req, res) => {
    res.set('Content-Type', promClient.register.contentType);
    res.end(await promClient.register.metrics());
});
```

#### Step 4: Add Metrics to Key Functions

**Update us-representatives.js:**

```javascript
const { 
    recordCacheHit, 
    recordCacheMiss, 
    recordExternalApiCall,
    recordZipLookup
} = require('./metrics');

async function zipToCongressionalDistrict(zipCode) {
    const cached = await getCachedDistrict(zipCode);
    if (cached) {
        recordCacheHit('district');
        return cached;
    }
    
    recordCacheMiss('district');
    
    const apiStart = Date.now();
    try {
        const apiUrl = `https://whoismyrepresentative.com/getall_mems.php?zip=${zipCode}&output=json`;
        const response = await fetch(apiUrl);
        const data = await response.json();
        
        const duration = (Date.now() - apiStart) / 1000;
        recordExternalApiCall('whoismyrepresentative', duration, true);
        
        // ... rest of logic ...
    } catch (error) {
        const duration = (Date.now() - apiStart) / 1000;
        recordExternalApiCall('whoismyrepresentative', duration, false);
        throw error;
    }
}

async function getRepresentativesByZipCode(zipCode) {
    // ... existing code ...
    
    const allReps = [...federalReps, ...stateReps];
    
    // Record metrics
    const districtInfo = await zipToCongressionalDistrict(zipCode);
    recordZipLookup(districtInfo.state, allReps.length);
    
    return allReps;
}
```

**Update ai-service.js:**

```javascript
const { recordAiQuery, recordExternalApiCall } = require('./metrics');

async function analyzeWithAI(userMessage, context = {}, type = 'representatives') {
    const aiStart = Date.now();
    
    try {
        const response = await groq.chat.completions.create({
            model: 'llama-3.3-70b-versatile',
            messages: [
                { role: 'system', content: SYSTEM_PROMPTS[type] || SYSTEM_PROMPTS.representatives },
                { role: 'user', content: userMessage }
            ]
        });
        
        const duration = (Date.now() - aiStart) / 1000;
        recordAiQuery(type, 'llama-3.3-70b-versatile', duration);
        recordExternalApiCall('groq', duration, true);
        
        return {
            success: true,
            response: response.choices[0].message.content,
            sources: extractSources(response.choices[0].message.content),
            metadata: response.usage
        };
    } catch (error) {
        const duration = (Date.now() - aiStart) / 1000;
        recordExternalApiCall('groq', duration, false);
        throw error;
    }
}
```

#### Step 5: Install Prometheus Server

**On Ubuntu Server:**

```bash
# Download Prometheus
cd /opt
sudo wget https://github.com/prometheus/prometheus/releases/download/v2.45.0/prometheus-2.45.0.linux-amd64.tar.gz
sudo tar xvfz prometheus-*.tar.gz
sudo mv prometheus-2.45.0.linux-amd64 prometheus

# Create Prometheus user
sudo useradd --no-create-home --shell /bin/false prometheus

# Create directories
sudo mkdir -p /etc/prometheus /var/lib/prometheus
sudo chown prometheus:prometheus /var/lib/prometheus

# Create Prometheus config
sudo nano /etc/prometheus/prometheus.yml
```

**Prometheus configuration:**
```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'workforce-democracy-backend'
    static_configs:
      - targets: ['localhost:3001']
    metrics_path: '/metrics'
    scrape_interval: 5s
```

**Create systemd service:**
```bash
sudo nano /etc/systemd/system/prometheus.service
```

```ini
[Unit]
Description=Prometheus
Wants=network-online.target
After=network-online.target

[Service]
User=prometheus
Group=prometheus
Type=simple
ExecStart=/opt/prometheus/prometheus \
    --config.file=/etc/prometheus/prometheus.yml \
    --storage.tsdb.path=/var/lib/prometheus/ \
    --web.console.templates=/opt/prometheus/consoles \
    --web.console.libraries=/opt/prometheus/console_libraries

[Install]
WantedBy=multi-user.target
```

**Start Prometheus:**
```bash
sudo systemctl daemon-reload
sudo systemctl start prometheus
sudo systemctl enable prometheus

# Verify Prometheus is running
sudo systemctl status prometheus
curl http://localhost:9090/-/healthy
```

#### Step 6: Install Grafana

```bash
# Add Grafana repository
sudo apt-get install -y software-properties-common
sudo add-apt-repository "deb https://packages.grafana.com/oss/deb stable main"
wget -q -O - https://packages.grafana.com/gpg.key | sudo apt-key add -

# Install Grafana
sudo apt-get update
sudo apt-get install grafana -y

# Start Grafana
sudo systemctl start grafana-server
sudo systemctl enable grafana-server

# Grafana will be available at http://localhost:3000
# Default credentials: admin/admin (change on first login)
```

#### Step 7: Configure Grafana Dashboards

**Access Grafana:** http://your-server:3000

**Add Prometheus Data Source:**
1. Configuration â†’ Data Sources â†’ Add data source
2. Select "Prometheus"
3. URL: `http://localhost:9090`
4. Save & Test

**Import Pre-built Dashboard:**
1. Create â†’ Import
2. Use dashboard ID: 1860 (Node Exporter Full)
3. Select Prometheus data source

**Create Custom Dashboard with Panels:**

**Panel 1: Request Rate**
```promql
rate(http_requests_total[5m])
```

**Panel 2: Response Time (95th percentile)**
```promql
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))
```

**Panel 3: Cache Hit Rate**
```promql
rate(cache_operations_total{result="hit"}[5m]) / 
rate(cache_operations_total[5m]) * 100
```

**Panel 4: External API Health**
```promql
rate(external_api_errors_total[5m])
```

**Panel 5: AI Query Duration (median)**
```promql
histogram_quantile(0.5, rate(ai_query_duration_seconds_bucket[5m]))
```

**Panel 6: ZIP Lookups by State (Top 10)**
```promql
topk(10, rate(zip_lookups_by_state_total[1h]))
```

**Panel 7: Representatives Returned Distribution**
```promql
histogram_quantile(0.5, rate(representatives_returned_count_bucket[5m]))
```

#### Step 8: Set Up Alerts

**Create alert in Grafana:**

**Alert 1: High Error Rate**
```
Condition: rate(http_requests_total{status_code=~"5.."}[5m]) > 10
Alert: Send notification if >10 errors per second for 5 minutes
```

**Alert 2: Slow API Response**
```
Condition: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 5
Alert: Send notification if 95th percentile response time >5 seconds
```

**Alert 3: External API Failures**
```
Condition: rate(external_api_errors_total[5m]) > 5
Alert: Send notification if >5 external API errors per second
```

**Alert 4: Low Cache Hit Rate**
```
Condition: rate(cache_operations_total{result="hit"}[5m]) / rate(cache_operations_total[5m]) < 0.5
Alert: Send notification if cache hit rate <50% for 10 minutes
```

**Configure notification channels:**
1. Alerting â†’ Notification channels
2. Add channel (Email, Slack, PagerDuty, Webhook)
3. Test notification

#### Step 9: Nginx Configuration for Grafana

**Add reverse proxy for Grafana:**

```nginx
# /etc/nginx/sites-available/workforce-monitoring
server {
    server_name monitoring.workforcedemocracyproject.org;

    location / {
        proxy_pass http://localhost:3000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    # Protect with basic auth or IP whitelist
    auth_basic "Monitoring Access";
    auth_basic_user_file /etc/nginx/.htpasswd;

    listen 443 ssl;
    ssl_certificate /etc/letsencrypt/live/monitoring.workforcedemocracyproject.org/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/monitoring.workforcedemocracyproject.org/privkey.pem;
}
```

**Create basic auth:**
```bash
sudo apt-get install apache2-utils
sudo htpasswd -c /etc/nginx/.htpasswd admin
# Enter password when prompted
```

### Dashboard Preview (Example Metrics)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Workforce Democracy Backend - Live Metrics                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚ Request Rate:        45 req/sec    â–â–‚â–ƒâ–…â–‡â–ˆâ–‡â–…â–ƒâ–‚â–                 â”‚
â”‚ Avg Response Time:   342ms         â–‚â–ƒâ–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–ƒâ–„â–ƒ                 â”‚
â”‚ Error Rate:          0.2%          â–â–â–â–â–â–â–â–â–â–â–                 â”‚
â”‚ Cache Hit Rate:      87.3%         â–†â–‡â–‡â–†â–‡â–‡â–‡â–†â–‡â–‡â–†                 â”‚
â”‚                                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ External APIs (Last Hour)                                       â”‚
â”‚                                                                 â”‚
â”‚ Congress.gov:        142 calls     Avg: 1.2s    Errors: 0      â”‚
â”‚ OpenStates:          89 calls      Avg: 0.8s    Errors: 1      â”‚
â”‚ Whoismyrepresentative: 45 calls    Avg: 1.5s    Errors: 0      â”‚
â”‚ Groq AI:             23 calls      Avg: 3.4s    Errors: 0      â”‚
â”‚                                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Top States by ZIP Lookups (24 hours)                            â”‚
â”‚                                                                 â”‚
â”‚ 1. California        1,234 lookups                              â”‚
â”‚ 2. Texas             987 lookups                                â”‚
â”‚ 3. New York          856 lookups                                â”‚
â”‚ 4. Florida           743 lookups                                â”‚
â”‚ 5. Illinois          621 lookups                                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Cost Estimation
- **Development Time:** 12-16 hours
- **Testing Time:** 4-6 hours
- **Infrastructure:** Prometheus + Grafana = ~200MB RAM, 10GB storage
- **Maintenance:** Medium (dashboard updates, alert tuning)

### Expected Insights

After implementation, you'll be able to answer:
- âœ… "What's our average API response time by endpoint?"
- âœ… "Which states have the most user activity?"
- âœ… "Is Congress.gov API slower today than usual?"
- âœ… "How many AI queries are we processing per hour?"
- âœ… "What's our cache effectiveness percentage?"
- âœ… "Are we approaching any external API rate limits?"

---

## 4. Testing Suite with Jest

### Current State
**Problem:** No automated tests means:
- âŒ Regressions not caught before production
- âŒ Refactoring is risky
- âŒ Difficult to verify bug fixes
- âŒ No documentation of expected behavior

### Proposed Solution: Jest Testing Framework

**Benefits:**
- âœ… Catch bugs before deployment
- âœ… Confidence in refactoring
- âœ… Living documentation of functionality
- âœ… CI/CD pipeline integration ready

### Implementation Plan

#### Step 1: Install Dependencies
```bash
npm install --save-dev jest supertest @types/jest
npm install --save-dev nock  # Mock HTTP requests
```

#### Step 2: Configure Jest

**Update package.json:**
```json
{
  "scripts": {
    "test": "jest --coverage",
    "test:watch": "jest --watch",
    "test:unit": "jest --testPathPattern=unit",
    "test:integration": "jest --testPathPattern=integration"
  },
  "jest": {
    "testEnvironment": "node",
    "coverageDirectory": "coverage",
    "collectCoverageFrom": [
      "*.js",
      "!server.js",
      "!coverage/**",
      "!node_modules/**"
    ],
    "coverageThreshold": {
      "global": {
        "branches": 70,
        "functions": 70,
        "lines": 70,
        "statements": 70
      }
    },
    "testMatch": [
      "**/__tests__/**/*.test.js",
      "**/?(*.)+(spec|test).js"
    ]
  }
}
```

#### Step 3: Create Test Directory Structure

```bash
mkdir -p __tests__/{unit,integration,fixtures}
```

#### Step 4: Unit Tests for us-representatives.js

**File:** `__tests__/unit/us-representatives.test.js`

```javascript
const nock = require('nock');
const {
    zipToCongressionalDistrict,
    getRepresentativesByZipCode
} = require('../../us-representatives');

describe('us-representatives.js - Unit Tests', () => {
    
    afterEach(() => {
        nock.cleanAll();
    });

    describe('zipToCongressionalDistrict()', () => {
        
        test('should return district for valid ZIP code', async () => {
            // Mock whoismyrepresentative.com API
            nock('https://whoismyrepresentative.com')
                .get('/getall_mems.php')
                .query({ zip: '10001', output: 'json' })
                .reply(200, {
                    results: [
                        {
                            name: 'Jerrold Nadler',
                            state: 'NY',
                            district: '12',
                            phone: '202-225-5635'
                        }
                    ]
                });

            const result = await zipToCongressionalDistrict('10001');

            expect(result).toEqual({
                state: 'NY',
                district: 12,
                zipCode: '10001',
                fallback: false
            });
        });

        test('should handle invalid ZIP code gracefully', async () => {
            nock('https://whoismyrepresentative.com')
                .get('/getall_mems.php')
                .query({ zip: '00000', output: 'json' })
                .reply(200, { results: [] });

            const result = await zipToCongressionalDistrict('00000');

            expect(result.fallback).toBe(true);
            expect(result.state).toBeDefined();
        });

        test('should cache district lookups', async () => {
            nock('https://whoismyrepresentative.com')
                .get('/getall_mems.php')
                .query({ zip: '90210', output: 'json' })
                .reply(200, {
                    results: [{ name: 'Judy Chu', state: 'CA', district: '28' }]
                });

            // First call - should hit API
            const result1 = await zipToCongressionalDistrict('90210');
            
            // Second call - should use cache (nock will fail if API called again)
            const result2 = await zipToCongressionalDistrict('90210');

            expect(result1).toEqual(result2);
        });

        test('should handle API timeout', async () => {
            nock('https://whoismyrepresentative.com')
                .get('/getall_mems.php')
                .query({ zip: '10001', output: 'json' })
                .delayConnection(16000) // Exceed 15s timeout
                .reply(200, {});

            await expect(zipToCongressionalDistrict('10001'))
                .rejects
                .toThrow();
        });
    });

    describe('getRepresentativesByZipCode()', () => {
        
        test('should return federal + state representatives', async () => {
            // Mock all required APIs
            nock('https://whoismyrepresentative.com')
                .get('/getall_mems.php')
                .query({ zip: '10001', output: 'json' })
                .reply(200, {
                    results: [{ name: 'Test Rep', state: 'NY', district: '12' }]
                });

            nock('https://api.congress.gov')
                .get('/v3/member')
                .query(true)
                .times(3)
                .reply(200, {
                    members: [
                        {
                            bioguideId: 'S000148',
                            name: 'Charles Schumer',
                            state: 'NY',
                            district: null,
                            partyName: 'Democratic',
                            terms: { item: [{ chamber: 'Senate' }] }
                        }
                    ]
                });

            nock('https://v3.openstates.org')
                .get('/people')
                .query(true)
                .reply(200, {
                    results: [
                        {
                            id: 'ocd-person/1',
                            name: 'State Rep 1',
                            party: 'Democratic',
                            current_role: { title: 'Senator', district: '10' }
                        }
                    ]
                });

            const result = await getRepresentativesByZipCode('10001');

            expect(result).toBeInstanceOf(Array);
            expect(result.length).toBeGreaterThan(0);
            
            // Check federal representatives
            const federalReps = result.filter(r => r.level === 'federal');
            expect(federalReps.length).toBeGreaterThanOrEqual(2); // At least 2 senators
            
            // Check state representatives
            const stateReps = result.filter(r => r.level === 'state');
            expect(stateReps.length).toBeGreaterThan(0);
        });

        test('should include website URLs for all representatives', async () => {
            // ... mock setup ...

            const result = await getRepresentativesByZipCode('10001');

            result.forEach(rep => {
                expect(rep).toHaveProperty('website');
                if (rep.website) {
                    expect(rep.website).toMatch(/^https?:\/\//);
                }
            });
        });
    });
});
```

#### Step 5: Unit Tests for ai-service.js

**File:** `__tests__/unit/ai-service.test.js`

```javascript
const nock = require('nock');
const aiService = require('../../ai-service');

describe('ai-service.js - Unit Tests', () => {
    
    afterEach(() => {
        nock.cleanAll();
    });

    describe('analyzeWithAI()', () => {
        
        test('should return AI response with sources', async () => {
            nock('https://api.groq.com')
                .post('/openai/v1/chat/completions')
                .reply(200, {
                    choices: [{
                        message: {
                            content: 'Analysis of representative voting record...\n\nSources:\n1. ProPublica voting database\n2. OpenSecrets campaign finance'
                        }
                    }],
                    usage: {
                        prompt_tokens: 150,
                        completion_tokens: 300,
                        total_tokens: 450
                    }
                });

            const result = await aiService.analyzeWithAI(
                'Tell me about Representative Smith',
                { representative: 'Smith' },
                'representatives'
            );

            expect(result).toHaveProperty('success', true);
            expect(result).toHaveProperty('response');
            expect(result).toHaveProperty('sources');
            expect(result).toHaveProperty('metadata');
            expect(result.sources).toBeInstanceOf(Array);
        });

        test('should use correct system prompt for chat type', async () => {
            const mockGroq = nock('https://api.groq.com')
                .post('/openai/v1/chat/completions', body => {
                    // Verify system prompt contains labor-specific content
                    return body.messages[0].content.includes('labor') ||
                           body.messages[0].content.includes('workers');
                })
                .reply(200, {
                    choices: [{ message: { content: 'Labor analysis...' } }],
                    usage: { total_tokens: 100 }
                });

            await aiService.analyzeWithAI(
                'Tell me about union rights',
                {},
                'labor'
            );

            expect(mockGroq.isDone()).toBe(true);
        });

        test('should handle Groq API errors gracefully', async () => {
            nock('https://api.groq.com')
                .post('/openai/v1/chat/completions')
                .reply(500, { error: 'Internal server error' });

            await expect(aiService.analyzeWithAI('Test query', {}, 'representatives'))
                .rejects
                .toThrow();
        });

        test('should generate compassionate fallback on error', async () => {
            const fallback = await aiService.generateCompassionateFallback(
                new Error('API unavailable')
            );

            expect(fallback).toBeTruthy();
            expect(typeof fallback).toBe('string');
            expect(fallback.length).toBeGreaterThan(50);
        });
    });

    describe('Source extraction', () => {
        
        test('should extract sources from AI response', () => {
            const response = `
                Analysis here...
                
                Sources:
                1. ProPublica - Representative voting record
                2. OpenSecrets.org - Campaign finance data
                3. Congress.gov - Bill sponsorships
            `;

            const sources = aiService.extractSources(response);

            expect(sources).toBeInstanceOf(Array);
            expect(sources.length).toBe(3);
            expect(sources[0]).toContain('ProPublica');
        });

        test('should prioritize independent journalism sources', () => {
            const sources = [
                'CNN - News coverage',
                'Democracy Now - Investigative report',
                'Fox News - Commentary'
            ];

            const prioritized = aiService.prioritizeSources(sources);

            expect(prioritized[0]).toContain('Democracy Now');
        });
    });
});
```

#### Step 6: Integration Tests

**File:** `__tests__/integration/api-endpoints.test.js`

```javascript
const request = require('supertest');
const app = require('../../server'); // Export app from server.js

describe('API Endpoints - Integration Tests', () => {
    
    describe('GET /health', () => {
        test('should return healthy status', async () => {
            const response = await request(app)
                .get('/health')
                .expect(200);

            expect(response.body).toHaveProperty('status', 'healthy');
            expect(response.body).toHaveProperty('groqConfigured');
        });
    });

    describe('GET /api/civic/representatives', () => {
        test('should return representatives for valid ZIP', async () => {
            const response = await request(app)
                .get('/api/civic/representatives')
                .query({ zipCode: '10001' })
                .expect(200);

            expect(response.body).toBeInstanceOf(Array);
            expect(response.body.length).toBeGreaterThan(0);
            
            const firstRep = response.body[0];
            expect(firstRep).toHaveProperty('name');
            expect(firstRep).toHaveProperty('party');
            expect(firstRep).toHaveProperty('level');
            expect(firstRep).toHaveProperty('website');
        });

        test('should return 400 for missing ZIP code', async () => {
            const response = await request(app)
                .get('/api/civic/representatives')
                .expect(400);

            expect(response.body).toHaveProperty('error');
        });

        test('should return 400 for invalid ZIP code format', async () => {
            const response = await request(app)
                .get('/api/civic/representatives')
                .query({ zipCode: 'invalid' })
                .expect(400);

            expect(response.body.error).toContain('valid 5-digit');
        });
    });

    describe('POST /api/backend/query', () => {
        test('should return AI response for valid query', async () => {
            const response = await request(app)
                .post('/api/backend/query')
                .send({
                    query: 'What are the duties of a representative?',
                    context: {},
                    chatType: 'representatives'
                })
                .expect(200);

            expect(response.body).toHaveProperty('success', true);
            expect(response.body).toHaveProperty('response');
            expect(response.body.response.length).toBeGreaterThan(50);
        }, 30000); // 30s timeout for AI query

        test('should return 400 for missing query', async () => {
            const response = await request(app)
                .post('/api/backend/query')
                .send({ context: {} })
                .expect(400);

            expect(response.body).toHaveProperty('error');
        });
    });

    describe('Rate Limiting', () => {
        test('should enforce rate limits on AI endpoints', async () => {
            // Make 11 rapid requests (limit is 10/minute)
            const requests = [];
            for (let i = 0; i < 11; i++) {
                requests.push(
                    request(app)
                        .post('/api/backend/query')
                        .send({ query: `Test ${i}`, context: {} })
                );
            }

            const responses = await Promise.all(requests);
            
            const rateLimitedResponses = responses.filter(r => r.status === 429);
            expect(rateLimitedResponses.length).toBeGreaterThan(0);
        }, 30000);
    });
});
```

#### Step 7: Test Fixtures

**File:** `__tests__/fixtures/representatives.json`

```json
{
  "ny_10001": {
    "zipCode": "10001",
    "district": {
      "state": "NY",
      "district": 12
    },
    "representatives": [
      {
        "name": "Charles Schumer",
        "party": "Democratic",
        "level": "federal",
        "chamber": "senate",
        "website": "https://www.schumer.senate.gov"
      },
      {
        "name": "Kirsten Gillibrand",
        "party": "Democratic",
        "level": "federal",
        "chamber": "senate",
        "website": "https://www.gillibrand.senate.gov"
      },
      {
        "name": "Jerrold Nadler",
        "party": "Democratic",
        "level": "federal",
        "chamber": "house",
        "district": "12",
        "website": "https://nadler.house.gov"
      }
    ]
  }
}
```

#### Step 8: Update server.js to Export App

**Add to end of server.js:**

```javascript
// Export app for testing
module.exports = app;

// Only start server if not in test environment
if (process.env.NODE_ENV !== 'test') {
    const PORT = process.env.PORT || 3001;
    app.listen(PORT, () => {
        console.log(`Server running on port ${PORT}`);
    });
}
```

#### Step 9: Add Test Scripts

**Add to package.json:**

```json
{
  "scripts": {
    "test": "NODE_ENV=test jest --coverage --verbose",
    "test:watch": "NODE_ENV=test jest --watch",
    "test:unit": "NODE_ENV=test jest --testPathPattern=unit --verbose",
    "test:integration": "NODE_ENV=test jest --testPathPattern=integration --verbose",
    "test:ci": "NODE_ENV=test jest --ci --coverage --maxWorkers=2"
  }
}
```

#### Step 10: Create GitHub Actions CI Pipeline

**File:** `.github/workflows/test.yml`

```yaml
name: Backend Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  test:
    runs-on: ubuntu-latest

    strategy:
      matrix:
        node-version: [16.x, 18.x, 20.x]

    steps:
    - uses: actions/checkout@v3
    
    - name: Use Node.js ${{ matrix.node-version }}
      uses: actions/setup-node@v3
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'npm'
    
    - name: Install dependencies
      run: npm ci
    
    - name: Run unit tests
      run: npm run test:unit
    
    - name: Run integration tests
      run: npm run test:integration
      env:
        GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
        CONGRESS_API_KEY: ${{ secrets.CONGRESS_API_KEY }}
        OPENSTATES_API_KEY: ${{ secrets.OPENSTATES_API_KEY }}
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage/coverage-final.json
        fail_ci_if_error: true
```

#### Step 11: Run Tests

```bash
# Run all tests with coverage
npm test

# Expected output:
# PASS  __tests__/unit/us-representatives.test.js
# PASS  __tests__/unit/ai-service.test.js
# PASS  __tests__/integration/api-endpoints.test.js
#
# Test Suites: 3 passed, 3 total
# Tests:       24 passed, 24 total
# Snapshots:   0 total
# Time:        15.234s
# Coverage:    78.3% statements, 72.1% branches, 81.5% functions, 77.9% lines

# Run only unit tests
npm run test:unit

# Run tests in watch mode (re-runs on file changes)
npm run test:watch
```

### Test Coverage Goals

| Module | Target Coverage | Priority |
|--------|----------------|----------|
| us-representatives.js | 80% | HIGH |
| ai-service.js | 75% | HIGH |
| server.js | 60% | MEDIUM |
| redis-service.js | 70% | MEDIUM |
| rate-limiter.js | 65% | LOW |

### Cost Estimation
- **Development Time:** 12-16 hours
- **Testing Time:** 4-6 hours
- **Maintenance:** Low (add tests for new features)
- **CI/CD:** Free (GitHub Actions: 2,000 minutes/month)

---

## 5. Archive Organization

### Current State
**File:** `/var/www/workforce-backend-OBSOLETE-20251102-210054.tar.gz` (3.8MB)  
**Location:** Root of /var/www/ directory  
**Status:** Unorganized, difficult to find

### Proposed Solution: Proper Archive Organization

**Benefits:**
- âœ… Clear separation from active code
- âœ… Easy to locate for future reference
- âœ… Prevents accidental use of old code

### Implementation Plan

#### Step 1: Create Archive Directory

```bash
# Create organized archive location
mkdir -p /var/www/workforce-democracy/backend/OLD_BACKEND_ARCHIVE

# Move backup archive
mv /var/www/workforce-backend-OBSOLETE-20251102-210054.tar.gz \
   /var/www/workforce-democracy/backend/OLD_BACKEND_ARCHIVE/

# Create README in archive directory
cat > /var/www/workforce-democracy/backend/OLD_BACKEND_ARCHIVE/README.md << 'EOF'
# Old Backend Archive

This directory contains archived versions of the backend that have been replaced.

## workforce-backend-OBSOLETE-20251102-210054.tar.gz

**Archive Date:** 2025-11-02 21:00:54 UTC  
**Size:** 3.8MB  
**Original Location:** `/var/www/workforce-backend/`

**Reason for Archival:**  
Duplicate backend directory that was missing critical features:
- âŒ No ai-service integration
- âŒ No us-representatives module
- âŒ Missing /api/backend/query endpoint
- âŒ Missing /api/photo-proxy endpoint

**Active Backend:**  
Current backend located at: `/var/www/workforce-democracy/backend/`

**To Restore (NOT RECOMMENDED):**
```bash
cd /var/www
tar -xzf /var/www/workforce-democracy/backend/OLD_BACKEND_ARCHIVE/workforce-backend-OBSOLETE-20251102-210054.tar.gz
```

**âš ï¸ WARNING:** This archive is for reference only. Do not use in production.
EOF

# Set proper permissions
chmod 644 /var/www/workforce-democracy/backend/OLD_BACKEND_ARCHIVE/*
chmod 755 /var/www/workforce-democracy/backend/OLD_BACKEND_ARCHIVE

# Verify
ls -lh /var/www/workforce-democracy/backend/OLD_BACKEND_ARCHIVE/
```

#### Step 2: Add to .gitignore

```bash
# Add to .gitignore (if using git)
echo "OLD_BACKEND_ARCHIVE/" >> /var/www/workforce-democracy/backend/.gitignore
```

#### Step 3: Document in Main README

**Add section to `/var/www/workforce-democracy/backend/README.md`:**

```markdown
## Archive Policy

Old backend versions are archived in `OLD_BACKEND_ARCHIVE/` for reference.

**Archive Location:** `./OLD_BACKEND_ARCHIVE/`  
**Retention Policy:** Keep for 6 months, then delete

**Current Archives:**
- `workforce-backend-OBSOLETE-20251102-210054.tar.gz` (3.8MB)
  - Archived: 2025-11-02
  - Delete After: 2025-05-02
```

### Cost Estimation
- **Development Time:** 15 minutes
- **Storage Impact:** 3.8MB (negligible)
- **Maintenance:** None

---

## ðŸ“Š Implementation Priority Recommendations

### Phase 1: Foundation (Week 1-2)
1. **Redis Caching** (Priority: HIGH)
   - Biggest performance impact
   - Enables horizontal scaling
   - Dependency for rate limiting
   
2. **Rate Limiting** (Priority: HIGH)
   - Protects external API quotas
   - Prevents abuse
   - Simple implementation

3. **Archive Organization** (Priority: LOW)
   - Quick win (15 minutes)
   - Good housekeeping

### Phase 2: Observability (Week 3-4)
4. **Prometheus & Grafana** (Priority: MEDIUM)
   - Critical for production monitoring
   - Enables proactive issue detection
   - More complex, but high value

### Phase 3: Quality Assurance (Week 5-6)
5. **Jest Testing Suite** (Priority: MEDIUM)
   - Enables confident refactoring
   - Catches regressions
   - CI/CD foundation

---

## ðŸ“ Success Metrics

### Redis Caching
- âœ… Cache hit rate >80%
- âœ… Average response time reduction >50%
- âœ… Zero cache-related outages for 30 days

### Rate Limiting
- âœ… Zero external API quota violations
- âœ… <0.1% requests blocked by rate limits (legitimate traffic)
- âœ… Successfully blocks >99% of abuse attempts

### Monitoring
- âœ… <1 hour mean time to detection (MTTD) for issues
- âœ… 100% uptime visibility (no blind spots)
- âœ… Alert fatigue rate <10% false positives

### Testing
- âœ… >75% code coverage across all modules
- âœ… Zero production bugs that would've been caught by tests
- âœ… <5 minute CI/CD pipeline execution time

---

## ðŸ”’ Security Considerations

### Redis Security
- Use strong passwords in production
- Bind to localhost only (unless clustering)
- Enable AUTH authentication
- Keep Redis updated (security patches)

### Prometheus/Grafana Security
- Enable basic auth or OAuth
- Use HTTPS for dashboard access
- Restrict admin endpoints to internal network
- Regular security updates

### Rate Limiting Security
- Use IP-based limiting cautiously (consider proxies/NAT)
- Implement captcha for borderline cases
- Log rate limit violations for abuse monitoring
- Consider user-based limits (not just IP)

---

## ðŸš€ Deployment Checklist

### Pre-Deployment
- [ ] All tests passing
- [ ] Code reviewed
- [ ] Documentation updated
- [ ] Environment variables configured
- [ ] Database backups created

### Deployment
- [ ] Deploy to staging first
- [ ] Smoke test critical endpoints
- [ ] Monitor error rates for 15 minutes
- [ ] Check Grafana dashboards
- [ ] Verify cache hit rates

### Post-Deployment
- [ ] Update README.md with changes
- [ ] Document any new configuration
- [ ] Update FUTURE_OPTIMIZATIONS.md status
- [ ] Send deployment notification
- [ ] Monitor for 24 hours

---

## ðŸ“ž Support & Questions

**Project Maintainer:** Workforce Democracy Team  
**Documentation:** This file (`FUTURE_OPTIMIZATIONS.md`)  
**Last Updated:** 2025-11-02

**Questions about implementation?**
1. Review this document thoroughly
2. Check existing implementation in active backend
3. Test in development environment first
4. Document any deviations from plan

---

## âœ… Completion Tracking

Use this checklist to track implementation progress:

### Redis Caching
- [ ] Install redis-server and ioredis npm package
- [ ] Create redis-service.js module
- [ ] Update us-representatives.js to use Redis
- [ ] Add cache monitoring endpoint
- [ ] Test cache hit/miss scenarios
- [ ] Deploy to production
- [ ] Monitor cache effectiveness for 1 week
- [ ] Document in README.md

### Rate Limiting
- [ ] Install express-rate-limit and rate-limit-redis
- [ ] Create rate-limiter.js module
- [ ] Apply limiters to all endpoints
- [ ] Add rate limit monitoring endpoint
- [ ] Test rate limit enforcement
- [ ] Deploy to production
- [ ] Monitor false positive rate for 1 week
- [ ] Document in README.md

### Monitoring
- [ ] Install Prometheus server
- [ ] Install Grafana
- [ ] Create metrics.js module
- [ ] Integrate metrics into server.js
- [ ] Add metrics to us-representatives.js
- [ ] Add metrics to ai-service.js
- [ ] Create Grafana dashboards
- [ ] Set up alerting rules
- [ ] Configure notification channels
- [ ] Test alerts
- [ ] Document dashboard access in README.md

### Testing
- [ ] Install Jest and testing dependencies
- [ ] Configure Jest in package.json
- [ ] Write unit tests for us-representatives.js
- [ ] Write unit tests for ai-service.js
- [ ] Write integration tests for API endpoints
- [ ] Set up GitHub Actions CI pipeline
- [ ] Achieve >75% code coverage
- [ ] Document testing procedures in README.md

### Archive Organization
- [ ] Create OLD_BACKEND_ARCHIVE directory
- [ ] Move backup archive to organized location
- [ ] Create archive README
- [ ] Update main README with archive policy
- [ ] Add to .gitignore

---

**End of Future Optimizations Document**

*This is a living document. Update as implementations are completed or plans change.*
