# ðŸ”§ FIX CHROMIUM DEPENDENCIES - v37.20.0.1

## ðŸš¨ **THE PROBLEM**

Your article scraper deployment failed with this error:

```
browserType.launch: Target page, context or browser has been closed
Failed to launch Chromium: libnspr4.so: cannot open shared object file: No such file or directory
```

**Root Cause:** Server is missing **system libraries** that Chromium needs to run.

**Missing Libraries:**
- `libnspr4.so` â† **The one in the error**
- `libnss3`, `libatk1.0-0`, `libgbm1`, and **20+ others**

This is a common issue when deploying Playwright/Chromium to fresh Ubuntu servers.

---

## âœ… **THE SOLUTION**

Install all required system libraries on your VPS.

---

## ðŸ“‹ **DEPLOYMENT COMMANDS (Copy-Paste Ready)**

### **Step 1: Install Missing System Libraries**

From your Mac terminal, run these commands **one at a time** (password prompt for each):

```bash
# Command 1: Update package lists
ssh root@185.193.126.13 'apt-get update'

# Command 2: Install Chromium dependencies
ssh root@185.193.126.13 'apt-get install -y libnss3 libnspr4 libatk1.0-0 libatk-bridge2.0-0 libcups2 libdrm2 libxkbcommon0 libxcomposite1 libxdamage1 libxfixes3 libxrandr2 libgbm1 libpango-1.0-0 libcairo2 libasound2 libatspi2.0-0'

# Command 3: Install screen utility (for background processes)
ssh root@185.193.126.13 'apt-get install -y screen'
```

**Password:** `YNWA1892LFC`

---

### **Step 2: Verify Chromium Can Launch**

Test that the fix worked:

```bash
ssh root@185.193.126.13 'cd /var/www/workforce-democracy/version-b/backend && node -e "const playwright = require(\"playwright\"); (async () => { console.log(\"ðŸ§ª Testing Chromium launch...\"); const browser = await playwright.chromium.launch({ headless: true }); const page = await browser.newPage(); console.log(\"âœ… Chromium launched successfully!\"); await browser.close(); console.log(\"âœ… Browser closed cleanly!\"); process.exit(0); })().catch(err => { console.error(\"âŒ Test failed:\", err.message); process.exit(1); });"'
```

**Expected Output:**
```
ðŸ§ª Testing Chromium launch...
âœ… Chromium launched successfully!
âœ… Browser closed cleanly!
```

---

### **Step 3: Test Article Scraper**

Now test the article scraper with a real article:

```bash
ssh root@185.193.126.13 'cd /var/www/workforce-democracy/version-b/backend && node -e "const { scrapeArticle } = require(\"./services/article-scraper-playwright.js\"); scrapeArticle(\"https://www.democracynow.org/2025/1/2/zohran_mamdani_new_york_city_mayor\").then(r => console.log(\"SUCCESS:\", r.title, r.fullText.substring(0,200))).catch(e => console.error(\"FAIL:\", e.message));"'
```

**Expected Output:**
```
SUCCESS: Zohran Mamdani on NYC Mayor Race [full article text showing...]
```

---

### **Step 4: Start Historical Seeder (Background)**

This will run for 2-3 hours in the background:

```bash
# Start screen session
ssh root@185.193.126.13 'screen -S seeder -dm bash -c "cd /var/www/workforce-democracy/version-b/backend && node scripts/seed-historical-articles.js > seed.log 2>&1"'
```

**Monitor Progress:**

```bash
# Watch the log file (live)
ssh root@185.193.126.13 'tail -f /var/www/workforce-democracy/version-b/backend/seed.log'

# Or reconnect to the screen session
ssh root@185.193.126.13
screen -r seeder
# Press Ctrl+A then D to detach
```

---

## ðŸ“Š **WHAT HAPPENS NEXT**

### **During Seeding (2-3 hours):**
- Searches RSS feeds for 24 key topics (Mamdani, AOC, Bernie, Medicare for All, etc.)
- Scrapes full article content with Playwright
- Indexes into MongoDB with metadata
- Target: **5,000+ articles**

### **After Seeding Complete:**

Check database article count:
```bash
ssh root@185.193.126.13 'cd /var/www/workforce-democracy/version-b/backend && node -e "const mongoose = require(\"mongoose\"); const Article = require(\"./models/Article\"); mongoose.connect(\"mongodb://localhost:27017/workforce_democracy\").then(async () => { const count = await Article.countDocuments(); console.log(\"Total articles:\", count); const sources = await Article.aggregate([{ \\$group: { _id: \"\\$source\", count: { \\$sum: 1 } } }, { \\$sort: { count: -1 } }]); console.log(\"By source:\", sources); process.exit(0); });"'
```

**Expected:** Shows 5,000+ articles across 7 sources.

---

## ðŸŽ¯ **USER-FACING IMPACT**

Once seeding completes, queries will be **dramatically better**:

**Before (v37.19.8.3):**
- âŒ "What are Mamdani's policies?" â†’ 3 sources
- âŒ Scraping failures for Democracy Now
- âŒ Shallow analysis

**After (v37.20.0):**
- âœ… "What are Mamdani's policies?" â†’ **10+ sources**
- âœ… Full article content (no scraping failures)
- âœ… Detailed policy analysis with:
  - Specific policy mechanisms
  - Direct quotes
  - Legislative details
  - Contradictions analysis

---

## ðŸ”§ **TROUBLESHOOTING**

### **If Chromium Test Still Fails:**

Try installing additional dependencies:
```bash
ssh root@185.193.126.13 'apt-get install -y fonts-liberation libappindicator3-1 libu2f-udev libvulkan1 xdg-utils'
```

### **If Seeder Crashes:**

Check the error in the log:
```bash
ssh root@185.193.126.13 'tail -100 /var/www/workforce-democracy/version-b/backend/seed.log'
```

Common fixes:
- **Out of memory:** Reduce batch size in `seed-historical-articles.js` (line 142: change 200 to 50)
- **MongoDB connection:** `sudo systemctl restart mongodb`
- **Timeout errors:** Increase timeout in `article-scraper-playwright.js` (line 89: 30000 â†’ 60000)

---

## ðŸ“š **NEXT STEPS**

After seeding completes:

1. âœ… **Set up hourly RSS monitoring** (so new articles are indexed automatically)
   ```bash
   ssh root@185.193.126.13 'crontab -e'
   # Add this line:
   0 * * * * cd /var/www/workforce-democracy/version-b/backend && node -e "require('./services/rss-monitor').monitorAllFeeds()" >> /var/log/rss-monitor.log 2>&1
   ```

2. âœ… **Test the system** with "What are Mamdani's policies?"

3. âœ… **Deploy to Version A** when Version B is stable and tested

---

## ðŸŽ‰ **SUMMARY**

**What This Fix Does:**
- âœ… Installs 20+ system libraries Chromium needs
- âœ… Enables Playwright scraper to work
- âœ… Unlocks full article scraping system

**Time Required:**
- Step 1-3: **5 minutes** (hands-on)
- Step 4: **2-3 hours** (automated, runs in background)

**Result:**
- âœ… **5,000+ articles** indexed
- âœ… **10+ sources** per query (instead of 3)
- âœ… **Detailed policy analysis** (no more "insufficient content")

---

**Version:** v37.20.0.1  
**Status:** Ready to deploy  
**Priority:** CRITICAL - Blocks article scraper system  
**Created:** 2025-12-01
